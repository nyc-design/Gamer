###############################################################################
# Gaming VM Docker Compose — PoC for 3DS Streaming
#
# Runs on a GPU VM (TensorDock, GCP, or any machine with NVIDIA GPU).
# Wolf handles Moonlight protocol, video encoding, and spawns the Azahar
# container when a user connects.
#
# Prerequisites on the host:
#   - NVIDIA GPU with driver installed
#   - Docker + NVIDIA Container Toolkit
#   - Ports 47984-48010 open (Moonlight protocol)
#   - ROMs placed in ~/roms/
#
# Usage:
#   docker compose build
#   docker compose up -d
#   # Then pair your iPhone Moonlight app to the VM's IP
###############################################################################

services:
  ###########################################################################
  # Wolf — Moonlight-compatible streaming server
  #
  # Handles: RTSP/RTP streaming, NVENC video encoding, PulseAudio capture,
  # virtual input devices (gamepad, touch), Docker app orchestration.
  #
  # When a Moonlight client connects and selects "Azahar 3DS", Wolf spawns
  # the azahar container with the correct GPU, audio, and display access.
  ###########################################################################
  wolf:
    # Use wolf-dual for dual-screen support (custom gst-wayland-display .so overlay).
    # Falls back to stock wolf:stable for single-screen mode.
    image: ${WOLF_IMAGE:-ghcr.io/games-on-whales/wolf:stable}
    environment:
      # Enable nvidia-container-runtime GPU injection.
      # With default-runtime=nvidia in daemon.json, these env vars trigger
      # the runtime to mount host CUDA/NVENC libs into the container.
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      # Use nvidia driver volume approach (per Wolf docs) — Wolf mounts this
      # volume into spawned app containers for GPU access
      - NVIDIA_DRIVER_VOLUME_NAME=nvidia-driver-vol
      - WOLF_RENDER_NODE=/dev/dri/renderD128
      - WOLF_LOG_LEVEL=INFO
      # Dual-screen: enable multi-output in gst-wayland-display.
      # This env var is checked by our forked waylanddisplaysrc at startup,
      # bypassing Wolf's hardcoded pipeline string. Harmless on stock Wolf.
      - GST_WD_MULTI_OUTPUT=1
      # Name of in-process interpipesink fed by waylanddisplaysecondary.
      - GST_WD_SECONDARY_SINK_NAME=secondary_video
    volumes:
      # Wolf state directory — MUST mount at /etc/wolf (not a subdirectory!)
      # Wolf inspects its own Docker mounts to resolve host paths for spawned
      # containers. It looks for a mount at /etc/wolf to pass config/state.
      - /etc/wolf:/etc/wolf:rw
      # Wolf needs Docker socket to spawn app containers
      - /var/run/docker.sock:/var/run/docker.sock:rw
      # Device access for GPU, input, and udev
      - /dev/:/dev/:rw
      - /run/udev:/run/udev:rw
      # NOTE: We do NOT mount nvidia-driver-vol into Wolf itself.
      # The nvidia container runtime (default-runtime=nvidia + NVIDIA_VISIBLE_DEVICES=all)
      # injects CUDA/NVENC libs automatically. Mounting the driver volume triggers
      # Wolf's 30-nvidia.sh init script which conflicts with the runtime's read-only
      # EGL mounts, causing a crash. Wolf still passes NVIDIA_DRIVER_VOLUME_NAME to
      # spawned app containers via its config.
    device_cgroup_rules:
      - 'c 13:* rmw'    # Input devices
      - 'c 244:* rmw'   # NVIDIA devices
    devices:
      - /dev/dri         # GPU render nodes
      - /dev/uinput      # Virtual input device creation
      - /dev/udmabuf     # DMA buffer allocation for Wayland compositor
      - /dev/nvidia-uvm
      - /dev/nvidia-uvm-tools
      - /dev/nvidia-caps/nvidia-cap1
      - /dev/nvidia-caps/nvidia-cap2
      - /dev/nvidiactl
      - /dev/nvidia0
      - /dev/nvidia-modeset
    network_mode: host
    restart: unless-stopped

  ###########################################################################
  # Wolf Dual-Screen — pre-built image with multi-output gst-wayland-display
  #
  # Pre-built and pushed to GHCR. To use dual-screen mode:
  #   export WOLF_IMAGE=ghcr.io/nyc-design/wolf-dual:latest
  #   docker compose up -d wolf
  #
  # To rebuild locally (requires Rust toolchain, ~15 min on 16 vCPU):
  #   docker build -f wolf/Dockerfile.wolf-dual -t ghcr.io/nyc-design/wolf-dual:latest wolf/
  ###########################################################################
  wolf-dual:
    build:
      context: ./wolf
      dockerfile: Dockerfile.wolf-dual
    image: ghcr.io/nyc-design/wolf-dual:latest
    profiles:
      - build-only

  ###########################################################################
  # Azahar 3DS Emulator — pre-built image
  #
  # This is NOT started by docker compose. Wolf spawns it on-demand when
  # a Moonlight client selects the "Azahar 3DS" app (defined in config.toml).
  #
  # We define it here only so `docker compose build` builds the image
  # and tags it as "gamer/azahar:poc" which the Wolf config references.
  ###########################################################################
  azahar:
    build:
      context: ./azahar
      dockerfile: Dockerfile
    image: gamer/azahar:poc
    # This container is managed by Wolf, not by compose.
    # Setting profiles prevents `docker compose up` from starting it directly.
    profiles:
      - build-only

volumes:
  nvidia-driver-vol:
    external: true
